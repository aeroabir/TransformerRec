{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence-to-sequence approach to recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read a sample interaction file and create the required input-output sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/recsys_data/RecSys/h_and_m_personalized_fashion_recommendation\"\n",
    "file_name = \"hnm_3w_sessionized.txt\" # \"hnm_big.txt\"\n",
    "seq_file_name = \"seq_\" + file_name\n",
    "\n",
    "inp_seq_len, tgt_seq_len = 10, 12\n",
    "colsep = \"\\t\"\n",
    "\n",
    "def get_ids(elems):\n",
    "    ids = []\n",
    "    for ii, e in enumerate(elems):\n",
    "        if e not in prod_dict[ii]:\n",
    "            prod_dict[ii][e] = len(prod_dict[ii]) + 1\n",
    "        ids.append(prod_dict[ii][e])\n",
    "    return ids\n",
    "\n",
    "def break_sessions(seqs):    \n",
    "    sids = sorted(list(set([x[-1] for x in seqs])))\n",
    "    temp = [[] for _ in range(len(sids))]\n",
    "    for seq in seqs:\n",
    "        temp[seq[-1]].append(seq[:-1])\n",
    "    return temp\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, seq_file_name)):\n",
    "    inp_file = os.path.join(data_dir, file_name)\n",
    "    sample = pd.read_csv(inp_file, sep=colsep, nrows=5)\n",
    "    ncol = sample.shape[1]\n",
    "\n",
    "    num_prod_dim = ncol - 3  # other than u, i, t\n",
    "    if num_prod_dim > 0:\n",
    "        prod_dict = [{} for _ in range(num_prod_dim)]\n",
    "        \n",
    "    User = defaultdict(list)\n",
    "    with open(os.path.join(data_dir, file_name), 'r') as fr:\n",
    "        for line in tqdm(fr):\n",
    "            if ncol == 3:\n",
    "                u, i, _ = line.rstrip().split(colsep)\n",
    "            elif ncol >= 4:\n",
    "                elems = line.rstrip().split(colsep)\n",
    "                u, i, t = elems[0], elems[1], elems[-1]\n",
    "                pdims = elems[2:-1]\n",
    "                pids = get_ids(pdims)\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            t = int(t)\n",
    "            if ncol >= 4:\n",
    "                User[u].append([i] + pids + [t])\n",
    "            else:\n",
    "                User[u].append(i)\n",
    "    print(f\"Read {len(User)} user interactions\")\n",
    "\n",
    "    with open(os.path.join(data_dir, seq_file_name), 'w') as fw:\n",
    "        for u in User:\n",
    "            seqs = break_sessions(User[u])\n",
    "            for ii in range(1, len(seqs)):\n",
    "                inp, tgt = seqs[ii-1], seqs[ii]\n",
    "                if len(inp) > inp_seq_len:\n",
    "                    inp = inp[-inp_seq_len:] # taking the last 12\n",
    "                if len(tgt) > tgt_seq_len:\n",
    "                    tgt = tgt[:tgt_seq_len]  # taking the first 12\n",
    "                inp = [str(ii[0]) for ii in inp]  # only the product-id\n",
    "                tgt = [str(ii[0]) for ii in tgt]  # always only the product-id\n",
    "                fw.write(\" \".join(inp) + \"\\t\" + \" \".join(tgt) + \"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the test file - only the last session for each user\n",
    "test_seq_file = \"seq_test_\" + file_name\n",
    "if not os.path.isfile(os.path.join(data_dir, test_seq_file)):\n",
    "    inp_file = os.path.join(data_dir, file_name)\n",
    "    sample = pd.read_csv(inp_file, sep=colsep, nrows=5)\n",
    "    ncol = sample.shape[1]\n",
    "\n",
    "    num_prod_dim = ncol - 3  # other than u, i, t\n",
    "    if num_prod_dim > 0:\n",
    "        prod_dict = [{} for _ in range(num_prod_dim)]\n",
    "        \n",
    "    User = defaultdict(list)\n",
    "    with open(os.path.join(data_dir, file_name), 'r') as fr:\n",
    "        for line in tqdm(fr):\n",
    "            if ncol == 3:\n",
    "                u, i, _ = line.rstrip().split(colsep)\n",
    "            elif ncol >= 4:\n",
    "                elems = line.rstrip().split(colsep)\n",
    "                u, i, t = elems[0], elems[1], elems[-1]\n",
    "                pdims = elems[2:-1]\n",
    "                pids = get_ids(pdims)\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            t = int(t)\n",
    "            if ncol >= 4:\n",
    "                User[u].append([i] + pids + [t])\n",
    "            else:\n",
    "                User[u].append(i)\n",
    "    print(f\"Read {len(User)} user interactions\")\n",
    "\n",
    "    with open(os.path.join(data_dir, test_seq_file), 'w') as fw:\n",
    "        for u in User:\n",
    "            seqs = break_sessions(User[u])\n",
    "            inp = seqs[-1]  # take the last session\n",
    "            inp = inp[:inp_seq_len]\n",
    "            inp = [str(ii[0]) for ii in inp]  # only the product-id\n",
    "            fw.write(\" \".join(inp) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the data in the required seq2seq form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoDataset:\n",
    "    def __init__(self, problem_type='reco'):\n",
    "        self.problem_type = 'reco'\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "    \n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    ## Step 1 and Step 2 \n",
    "    def preprocess_sentence_text(self, w):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    \n",
    "    def preprocess_sentence(self, w):\n",
    "        w = w.lower().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    def create_dataset(self, path, num_examples=None):\n",
    "        # path : path to spa-eng.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "        print(f\"Read {len(lines)} examples\")\n",
    "        if num_examples:\n",
    "            word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "        else:\n",
    "            word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n",
    "\n",
    "        return zip(*word_pairs)\n",
    "\n",
    "    # Step 3 and Step 4\n",
    "    def tokenize(self, lang):\n",
    "        # lang = list of sentences in a language\n",
    "        \n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        # creating cleaned input, output pairs\n",
    "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
    "\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def load_dataset_common(self, path, num_examples=None):\n",
    "        # creating cleaned input, output pairs with the same tokenizer\n",
    "        inp_lang, targ_lang = self.create_dataset(path, num_examples)\n",
    "        \n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(inp_lang + targ_lang)\n",
    "        \n",
    "        input_tensor = lang_tokenizer.texts_to_sequences(inp_lang) \n",
    "        target_tensor = lang_tokenizer.texts_to_sequences(targ_lang)\n",
    "        \n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, padding='pre')\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='pre')\n",
    "\n",
    "        return input_tensor, target_tensor, lang_tokenizer\n",
    "\n",
    "    def call(self, file_path, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
    "        input_tensor, target_tensor, tokenizer = self.load_dataset_common(file_path, num_examples)\n",
    "        self.inp_lang_tokenizer, self.targ_lang_tokenizer = tokenizer, tokenizer\n",
    "        \n",
    "        print(\"Example input:\", input_tensor[0])\n",
    "        print(\"Example target:\", target_tensor[0])\n",
    "        print(\"TENSOR SHAPE\", input_tensor.shape, target_tensor.shape)\n",
    "        \n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "        print(\"TRAIN:\", input_tensor_train.shape, target_tensor_train.shape)\n",
    "        print(\"VALID:\", input_tensor_val.shape, target_tensor_val.shape)\n",
    "        num_train, num_val = input_tensor_train.shape[0], input_tensor_val.shape[0]\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        \n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer, num_train, num_val\n",
    "    \n",
    "    def get_test_data(self, file_path, num_examples, BUFFER_SIZE, BATCH_SIZE, inp_seq_len):\n",
    "        \n",
    "        lines = io.open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "        print(f\"Read {len(lines)} test examples\")\n",
    "        sentences = [self.preprocess_sentence(l) for l in lines]\n",
    "        \n",
    "        # we take the target as input for \n",
    "        targ_lang = [x.split()[1:-1] for x in sentences]\n",
    "        targ_lang = [x[:inp_seq_len] for x in targ_lang]\n",
    "        targ_lang = [' '.join(x) for x in targ_lang]\n",
    "        targ_lang = ['<start> ' + x + ' <end>' for x in targ_lang]\n",
    "        target_tensor = self.inp_lang_tokenizer.texts_to_sequences(targ_lang)\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='pre')\n",
    "        print(target_tensor.shape)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(target_tensor)\n",
    "        test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        \n",
    "        return test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 71460 examples\n",
      "Example input: [  0   0   0   0   0   0   0   0   0   2 945   3]\n",
      "Example target: [    0     0     0     0     0     0     0     0     2 11992 13838  3548\n",
      "    42     3]\n",
      "TENSOR SHAPE (71460, 12) (71460, 14)\n",
      "TRAIN: (57168, 12) (57168, 14)\n",
      "VALID: (14292, 12) (14292, 14)\n",
      "Total 57168 training and 14292 validation examples\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 71460 # 1362281\n",
    "BATCH_SIZE = 256\n",
    "num_examples = None\n",
    "file_path = os.path.join(data_dir, seq_file_name)\n",
    "\n",
    "dataset_creator = RecoDataset('reco')\n",
    "train_dataset, val_dataset, inp_lang, targ_lang, num_train, num_val = dataset_creator.call(file_path, num_examples, BUFFER_SIZE, BATCH_SIZE)\n",
    "print(f\"Total {num_train} training and {num_val} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 48709 test examples\n",
      "(48709, 12)\n"
     ]
    }
   ],
   "source": [
    "test_file_path = os.path.join(data_dir, test_seq_file)\n",
    "test_data = dataset_creator.get_test_data(test_file_path, num_examples, BUFFER_SIZE, BATCH_SIZE, inp_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 12]), TensorShape([256, 14]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "steps_per_epoch = num_train//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_input, max_length_target, vocab_size_input, vocab_size_target, steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 14, 21464, 21464, 223)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length_input, max_length_target, vocab_size_input, vocab_size_target, steps\")\n",
    "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        ##-------- LSTM layer in Encoder ------- ##\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (256, 12, 512)\n",
      "Encoder h vecotr shape: (batch size, units) (256, 512)\n",
      "Encoder c vector shape: (batch size, units) (256, 512)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.attention_type = attention_type\n",
    "    \n",
    "    # Embedding Layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    #Final Dense layer on which softmax will be applied\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "   \n",
    "\n",
    "\n",
    "    # Sampler\n",
    "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "    \n",
    "  def build_rnn_cell(self, batch_sz):\n",
    "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "    return rnn_cell\n",
    "\n",
    "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "    if(attention_type=='bahdanau'):\n",
    "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "    else:\n",
    "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "    return decoder_initial_state\n",
    "\n",
    "\n",
    "  def call(self, inputs, initial_state):\n",
    "    x = self.embedding(inputs)\n",
    "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (256, 13, 21464)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './reco_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel(true, pred):\n",
    "    return 1 if true == pred else 0\n",
    "\n",
    "\n",
    "def precision_k(actual, predicted, k) -> float:\n",
    "    actual_set = set(actual[:k])\n",
    "    predicted_set = set(predicted[:k])\n",
    "    precision_k_value = len(actual_set & predicted_set) / k\n",
    "\n",
    "    return precision_k_value\n",
    "\n",
    "\n",
    "def mAP_k(actual, predicted) -> float:\n",
    "    # actual = row['valid_true'].split() # prediction_string --> prediction list\n",
    "    # predicted = row['valid_pred'].split() # prediction_string --> prediction list\n",
    "\n",
    "    M = min(len(actual), len(predicted))\n",
    "    K = min(M, 12)\n",
    "\n",
    "    if M == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        score = 0\n",
    "        for k in range(1, K + 1):\n",
    "            precision_k_value = precision_k(actual, predicted, k)\n",
    "\n",
    "            score += precision_k_value * rel(actual[k - 1], predicted[k - 1])\n",
    "        return score\n",
    "\n",
    "\n",
    "def map_batch(label, prediction):\n",
    "    \"\"\"\n",
    "    label: (batch, 12)\n",
    "    prediction: (batch, 12)\n",
    "    \"\"\"\n",
    "    pred = prediction.numpy()\n",
    "    label = label.numpy()\n",
    "    maps = []\n",
    "    for ii in range(prediction.shape[0]):\n",
    "        l_ii = [x for x in label[ii,:] if x not in [0, 2, 3]]\n",
    "        p_ii = [x for x in pred[ii,:] if x not in [2, 3]]\n",
    "        if len(p_ii) > 0:\n",
    "            maps.append(mAP_k(l_ii, p_ii))\n",
    "        else:\n",
    "            maps.append(0)\n",
    "    return np.mean(maps)\n",
    "\n",
    "\n",
    "def eval(dataset, encoder, decoder):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    all_maps = []\n",
    "    for (inp, targ) in tqdm(dataset):\n",
    "\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        prediction = tf.argmax(logits, axis=-1)\n",
    "        mapr = map_batch(real, prediction)\n",
    "        all_maps.append(mapr)\n",
    "    return np.mean(all_maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epochs with 223 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:39,  2.25it/s]\n",
      "55it [00:11,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 2.1180, val-MAP 0.0007\n",
      "Time taken for 1 epoch 110.65342926979065 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:27,  2.54it/s]\n",
      "55it [00:12,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 1.8303, val-MAP 0.0063\n",
      "Time taken for 1 epoch 100.14052391052246 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:26,  2.57it/s]\n",
      "55it [00:12,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 1.7859, val-MAP 0.0075\n",
      "Time taken for 1 epoch 99.86541700363159 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.48it/s]\n",
      "55it [00:13,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 1.7599, val-MAP 0.0079\n",
      "Time taken for 1 epoch 103.39815473556519 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:28,  2.52it/s]\n",
      "55it [00:13,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 1.7304, val-MAP 0.0079\n",
      "Time taken for 1 epoch 102.30819010734558 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.47it/s]\n",
      "55it [00:13,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 1.6973, val-MAP 0.0100\n",
      "Time taken for 1 epoch 103.92908501625061 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:29,  2.50it/s]\n",
      "55it [00:13,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 1.6557, val-MAP 0.0105\n",
      "Time taken for 1 epoch 102.73507618904114 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:29,  2.49it/s]\n",
      "55it [00:13,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 1.6059, val-MAP 0.0145\n",
      "Time taken for 1 epoch 103.18530130386353 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:28,  2.52it/s]\n",
      "55it [00:13,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 1.5516, val-MAP 0.0176\n",
      "Time taken for 1 epoch 102.21515345573425 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.47it/s]\n",
      "55it [00:13,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 1.4943, val-MAP 0.0188\n",
      "Time taken for 1 epoch 104.09590363502502 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:29,  2.48it/s]\n",
      "55it [00:13,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss 1.4304, val-MAP 0.0227\n",
      "Time taken for 1 epoch 103.65496921539307 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:31,  2.45it/s]\n",
      "55it [00:13,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 1.3594, val-MAP 0.0272\n",
      "Time taken for 1 epoch 105.13080525398254 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:29,  2.49it/s]\n",
      "55it [00:13,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss 1.2819, val-MAP 0.0284\n",
      "Time taken for 1 epoch 102.95392656326294 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:32,  2.42it/s]\n",
      "55it [00:13,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 1.2017, val-MAP 0.0315\n",
      "Time taken for 1 epoch 105.64360928535461 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.47it/s]\n",
      "55it [00:13,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss 1.1240, val-MAP 0.0318\n",
      "Time taken for 1 epoch 103.76611638069153 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.46it/s]\n",
      "55it [00:13,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 1.0490, val-MAP 0.0335\n",
      "Time taken for 1 epoch 104.50241160392761 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.46it/s]\n",
      "55it [00:13,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss 0.9782, val-MAP 0.0344\n",
      "Time taken for 1 epoch 104.66958284378052 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:31,  2.43it/s]\n",
      "55it [00:13,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.9117, val-MAP 0.0354\n",
      "Time taken for 1 epoch 105.65183734893799 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.45it/s]\n",
      "55it [00:13,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss 0.8505, val-MAP 0.0349\n",
      "Time taken for 1 epoch 104.71269512176514 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:31,  2.44it/s]\n",
      "55it [00:13,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss 0.7932, val-MAP 0.0341\n",
      "Time taken for 1 epoch 104.98081827163696 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:29,  2.48it/s]\n",
      "55it [00:13,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss 0.7396, val-MAP 0.0335\n",
      "Time taken for 1 epoch 103.26875376701355 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:30,  2.48it/s]\n",
      "55it [00:13,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss 0.6917, val-MAP 0.0334\n",
      "Time taken for 1 epoch 103.43218040466309 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [01:28,  2.53it/s]\n",
      "55it [00:13,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum patience (5) reached ... exiting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "print(f\"Training for {EPOCHS} epochs with {steps_per_epoch} steps per epoch\")\n",
    "best_val = 0.0\n",
    "patience, max_patience = 0, 5\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "    for (inp, targ) in tqdm(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "#     if batch % 10 == 0:\n",
    "#       print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                                    batch,\n",
    "#                                                    batch_loss.numpy()))\n",
    "\n",
    "    # evaluate validation data\n",
    "    map_val = eval(val_dataset, encoder, decoder)\n",
    "    if map_val > best_val:\n",
    "        best_val = map_val\n",
    "        print(\"Performance improved ... saving the model\")\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == max_patience:\n",
    "            print(f\"Maximum patience ({max_patience}) reached ... exiting!\")\n",
    "            break\n",
    "            \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "    print('Epoch {} Loss {:.4f}, val-MAP {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch, map_val))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf-addons BasicDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "    sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='pre')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "    end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "    result = evaluate_sentence(sentence)\n",
    "    print(result)\n",
    "    result = targ_lang.sequences_to_texts(result)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2c2bbe54d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[879 190 190 190 693 693 693 693 693 693 693 693   3]]\n",
      "Input: 1186\n",
      "Predicted translation: ['851 209 209 209 687 687 687 687 687 687 687 687 <end>']\n"
     ]
    }
   ],
   "source": [
    "translate('1186')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 640 2138  666  607   39   39 3552 3552  188  188  188  188    3]]\n",
      "Input: 13112 16042 3871 35\n",
      "Predicted translation: ['555 2159 587 545 44 44 3353 3353 164 164 164 164 <end>']\n"
     ]
    }
   ],
   "source": [
    "translate('13112 16042 3871 35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_sentence(sentences):\n",
    "    sentences = [dataset_creator.preprocess_sentence(sentence) for sentence in sentences]\n",
    "    inputs = inp_lang.texts_to_sequences(sentences)\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=inp_seq_len, padding='pre')\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "    end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n",
    "                                     start_tokens = start_tokens,\n",
    "                                     end_token= end_token,\n",
    "                                     initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def translate_multiple(sentences):\n",
    "    result = evaluate_multiple_sentence(sentences)\n",
    "    result = targ_lang.sequences_to_texts(result)\n",
    "    result = [[r for r in res.split() if r != \"<end>\"] for res in result]\n",
    "    result = [r[:tgt_seq_len] for r in result]\n",
    "    result = [' '.join(r) for r in result]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['851 209 209 209 1146 1146 1301 1301 4584 284 1448',\n",
       " '555 555 555 130 130 545 545 572 1719 572 1590']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_multiple(['1186', '13112 16042 3871 35'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seq_test_hnm_3w_sessionized.txt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/recsys_data/RecSys/h_and_m_personalized_fashion_recommendation/seq_test_hnm_3w_sessionized.txt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 48709 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48709/48709 [1:34:12<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 48709 lines in /recsys_data/RecSys/h_and_m_personalized_fashion_recommendation/seq_test_hnm_3w_sessionized.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = io.open(test_seq_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(f\"Read {len(lines)} test examples\")\n",
    "batch_size, count = 32, 0\n",
    "with open(os.path.join(data_dir, 'seq_test_pred.txt'), 'w') as fw:\n",
    "    for line in tqdm(lines):\n",
    "        result = evaluate_sentence(line)\n",
    "        result = targ_lang.sequences_to_texts(result)[0]\n",
    "        result = [r for r in result.split() if r != \"<end>\"]\n",
    "        result = result[:tgt_seq_len]\n",
    "        result = ' '.join(result)\n",
    "        fw.write(result + '\\n')\n",
    "        count += 1\n",
    "print(f\"Written {count} lines in {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 48709 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1523/1523 [00:00<00:00, 1272495.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 0 lines in /recsys_data/RecSys/h_and_m_personalized_fashion_recommendation/seq_test_hnm_3w_sessionized.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = io.open(test_seq_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(f\"Read {len(lines)} test examples\")\n",
    "batch_size, count = 32, 0\n",
    "with open(os.path.join(data_dir, 'seq_test_pred.txt'), 'w') as fw:\n",
    "    for ii in tqdm(range(0, len(lines), batch_size)):\n",
    "        end = ii + batch_size\n",
    "        if end > len(lines):\n",
    "            end = len(lines)\n",
    "        inps = lines[ii:end]\n",
    "#         result = translate_multiple(inps)\n",
    "#         for res in result:\n",
    "#             fw.write(res + '\\n')\n",
    "#             count += 1\n",
    "print(f\"Written {count} lines in {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12873',\n",
       " '5345 679',\n",
       " '7437 10120 10565 14782 1336 857 12314 1369 2335 2335',\n",
       " '180',\n",
       " '15']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf-addons BeamSearch Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='pre')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "  dec_h = enc_h\n",
    "  dec_c = enc_c\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  # From official documentation\n",
    "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "  print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "  # Instantiate BeamSearchDecoder\n",
    "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "  # The final beam predictions are stored in outputs.predicted_id\n",
    "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "  \n",
    "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "  \n",
    "  return final_outputs.numpy(), beam_scores.numpy()\n",
    "\n",
    "def beam_translate(sentence):\n",
    "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = targ_lang.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('<end>')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 102, 1024)\n",
      "(1, 3, 13) (1, 3, 13)\n",
      "(3, 13) (3, 13)\n",
      "Input: 75 76 77 78 79\n",
      "1 Predicted translation: 24621 24622 24622 20323 24623 24624 468 8919 24625 15315 24622 24626   -34.05118942260742\n",
      "2 Predicted translation: 21809 21809 31703 411 34156 1546 1546 1546 29535 6505 21752 11602   -36.71038055419922\n",
      "3 Predicted translation: 21809 21809 31703 411 34156 1546 1546 1546 29535 6505 6505 2549   -73.9906997680664\n"
     ]
    }
   ],
   "source": [
    "beam_translate('75 76 77 78 79')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "x[:3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
