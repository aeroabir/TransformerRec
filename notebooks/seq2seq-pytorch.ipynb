{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "\n",
    "import torchtext\n",
    "# from torchtext.legacy.datasets import Multi30k\n",
    "# from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# import spacy\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer_pyt import Encoder, Decoder, Seq2Seq, Seq2SeqMulti\n",
    "from transformer_pyt import train, evaluate\n",
    "from utils import get_session_data, build_vocab_from_seqs, data_process_meta, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0924243001',\n",
       " '0924243002',\n",
       " '0923758001',\n",
       " '0918522001',\n",
       " '0909370001',\n",
       " '0866731001',\n",
       " '0751471001',\n",
       " '0915529003',\n",
       " '0915529005',\n",
       " '0448509014',\n",
       " '0762846027',\n",
       " '0714790020']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/recsys_data/RecSys/h_and_m_personalized_fashion_recommendation\"\n",
    "file_name = \"hnm_7w_sessionized.txt\" # \"hnm_big.txt\"\n",
    "model_name = file_name.split(\".\")[0] + \".pt\"\n",
    "seq_file_name = \"seq_\" + file_name\n",
    "test_seq_file = \"seq_test_\" + file_name\n",
    "colsep = \"\\t\"\n",
    "\n",
    "inp_seq_len, tgt_seq_len = 12, 12\n",
    "BATCH_SIZE = 256\n",
    "num_examples = None\n",
    "file_path = os.path.join(data_dir, seq_file_name)\n",
    "test_file_path = os.path.join(data_dir, test_seq_file)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# en_tokenizer = get_tokenizer(language='en')\n",
    "\n",
    "tokens = tokenizer('0924243001 0924243002 0923758001 0918522001 0909370001 0866731001 0751471001 0915529003 0915529005 0448509014 0762846027 0714790020')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the sequence information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1332519it [00:07, 167655.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 144202 user interactions\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 'prod'])\n"
     ]
    }
   ],
   "source": [
    "inp_file = os.path.join(data_dir, file_name)\n",
    "\n",
    "all_seqs, prod_dict = get_session_data(inp_file, inp_seq_len=inp_seq_len, tgt_seq_len=tgt_seq_len)\n",
    "print(all_seqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278172, 222537, 55635)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = build_vocab_from_seqs(all_seqs['prod'], tokenizer)\n",
    "all_data = data_process_meta(all_seqs, tokenizer, src_vocab)\n",
    "train_data, val_data = train_test_split(all_data, test_size=0.2)\n",
    "# test_data = data_process(test_file_path, tokenizer, src_vocab, test_flag=True)\n",
    "len(all_data), len(train_data), len(val_data)#, len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['<unk>'], src_vocab['<pad>'], src_vocab['<bos>'], src_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 454, 5189, 2871,  788, 2764],\n",
       "         [   4,   11,   11,    4,    9],\n",
       "         [   1,    1,    1,    1,    2],\n",
       "         [   2,    2,    7,    2,    2],\n",
       "         [   2,   20,   12,   20,    2],\n",
       "         [  16,   16,   16,   16,   16],\n",
       "         [   2,    2,    2,    2,    2],\n",
       "         [   2,    2,    2,    2,    2],\n",
       "         [  14,   14,   14,   14,   14],\n",
       "         [   5,    5,    5,    5,    5]]),\n",
       " tensor([ 2105,  2105, 15553, 15553,   437,   128,  2955,   128,  1210,  1210,\n",
       "           437,  2955]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = src_vocab['<pad>']\n",
    "BOS_IDX = src_vocab['<bos>']\n",
    "EOS_IDX = src_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    inp_batch, meta_batch, tgt_batch = [], [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        n = de_item.shape[0]\n",
    "        before = torch.unsqueeze(torch.tensor([BOS_IDX] * n), 1)\n",
    "        after = torch.unsqueeze(torch.tensor([EOS_IDX] * n), 1)\n",
    "        total = torch.cat([before, de_item, after], dim=1)\n",
    "        total = total.permute(1, 0)\n",
    "        inp_batch.append(total)\n",
    "        tgt_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    \n",
    "#     print(torch.cat(inp_batch, dim=0).shape)\n",
    "    inp_batch = pad_sequence(inp_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return inp_batch, tgt_batch\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iterator = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension of each product attribute\n",
    "prod_dict_dims = [max(prod_dict[k].values()) for k in range(len(prod_dict))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple encoders - one for each product attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HID_DIM = 128\n",
    "HID_DIM2 = 32\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 128\n",
    "DEC_PF_DIM = 128\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "OUTPUT_DIM = len(src_vocab)\n",
    "\n",
    "encs = torch.nn.ModuleList()\n",
    "prod_enc = Encoder(input_dim=len(src_vocab), \n",
    "                   hid_dim=HID_DIM, \n",
    "                   n_layers=ENC_LAYERS, \n",
    "                   n_heads=ENC_HEADS, \n",
    "                   pf_dim=ENC_PF_DIM, \n",
    "                   dropout=ENC_DROPOUT, \n",
    "                   device=device)\n",
    "encs.append(prod_enc)\n",
    "total_dim = HID_DIM\n",
    "for pdim in prod_dict_dims:\n",
    "    enc_p = Encoder(input_dim=pdim, \n",
    "                   hid_dim=HID_DIM2, \n",
    "                   n_layers=ENC_LAYERS, \n",
    "                   n_heads=ENC_HEADS, \n",
    "                   pf_dim=ENC_PF_DIM, \n",
    "                   dropout=ENC_DROPOUT, \n",
    "                   device=device)\n",
    "    encs.append(enc_p)\n",
    "    total_dim += HID_DIM2\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = 0\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "model = Seq2SeqMulti(encs, dec, total_dim, HID_DIM, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,317,894 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:16,  3.04it/s]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_map = 0\n",
    "patience, max_patience = 0, 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, device)\n",
    "    valid_loss, valid_map = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_map > best_valid_map:\n",
    "        best_valid_map = valid_map\n",
    "        torch.save(model.state_dict(), 'reco-model.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience == max_patience:\n",
    "        print(\"Maximum patience reached ... exiting!\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | patience {patience}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val. MAP: {valid_map:7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Product Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71460, 57168, 14292, 48709)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = build_vocab_from_file(file_path, tokenizer)\n",
    "all_data = data_process(file_path, tokenizer, src_vocab)\n",
    "train_data, val_data = train_test_split(all_data, test_size=0.2)\n",
    "test_data = data_process(test_file_path, tokenizer, src_vocab, test_flag=True)\n",
    "len(all_data), len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(src_vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = src_vocab['<pad>']\n",
    "BOS_IDX = src_vocab['<bos>']\n",
    "EOS_IDX = src_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iterator = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = 0\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, src_vocab, trg_vocab, model, device, max_len = tgt_seq_len):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer(sentence)\n",
    "    tokens = [src_vocab['<bos>']] + tokens + [src_vocab['<eos>']]\n",
    "    src_indexes = [src_vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_vocab.stoi['<bos>']]\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_vocab.stoi['<eos>']:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1566', '<eos>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, _ = predict('13112 16042 3871 35', src_vocab, src_vocab, model, device)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (torch)",
   "language": "python",
   "name": "py37_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
